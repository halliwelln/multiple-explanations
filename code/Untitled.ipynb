{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "knowing-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join('..','data','experiment_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-association",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = 'Very intuitive, an explanation I could give or expect'\n",
    "second_best = 'Intuitive'\n",
    "middle = 'Neither intuitive or unintuitive'\n",
    "second_worst = 'Unintuitive'\n",
    "worst = 'Not intuitive at all, not an explanation I would give or expect'\n",
    "\n",
    "data = data.replace(best,5).replace(second_best,4).replace(middle,3).replace(second_worst,2).replace(worst,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_questions = data.iloc[:,1:4]\n",
    "filter_questions.columns = ['Native_Language', 'Nationality','Gender']\n",
    "#filter_questions = filter_questions.iloc[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-motion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-palestinian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter_questions[filter_questions['Native_Language'] == 'English']\n",
    "filter_questions['Native_Language'] = filter_questions['Native_Language'].apply(lambda x:x.title())\n",
    "filter_questions['Nationality'] = filter_questions['Nationality'].apply(lambda x:x.title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_questions.groupby(by=['Nationality','Native_Language']).agg(['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in data.columns if \n",
    "        (col != 'Timestamp') and ('Optional' not in col) \n",
    "                and (not col.startswith('Do') and (not col.startswith('What'))\n",
    "                and (not col.startswith('Any')) and (not col.startswith('Unnamed')))]\n",
    "\n",
    "#drop great grandparent\n",
    "cols = cols[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = data.loc[:,cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = [\n",
    "    'aunt_aunt_brother',\n",
    "    'aunt_grandparent_child',\n",
    "    'aunt_sister_parent',\n",
    "    'aunt_aunt_sister',\n",
    "    'uncle_brother_uncle',\n",
    "    'uncle_grandparent_child',\n",
    "    'uncle_parent_brother',\n",
    "    'uncle_uncle_sister',\n",
    "    'spouse_spouse',\n",
    "    'spouse_child_parent',\n",
    "    'brother_uncle_child',\n",
    "    'brother_parent_child',\n",
    "    'brother_sister_brother',\n",
    "    'brother_aunt_aunt',\n",
    "    'brother_grandparent_grandparent',\n",
    "    'brother_parent_parent',\n",
    "    'brother_sister_sister',\n",
    "    'brother_uncle_uncle',\n",
    "    'sister_brother',\n",
    "    'sister_brother_sister',\n",
    "    'sister_child_aunt',\n",
    "    'sister_parent_child',\n",
    "    'sister_aunt_aunt',\n",
    "    'sister_brother_brother',\n",
    "    'sister_grandparent_grandparent',\n",
    "    'sister_parent_parent',\n",
    "    'sister_uncle_uncle',\n",
    "    'grandparent_brother_grandparent',\n",
    "    'grandparent_grandparent_spouse',\n",
    "    'grandparent_parent_parent',\n",
    "    'grandparent_sister_grandparent',\n",
    "    'grandparent_aunt_child',\n",
    "    'grandparent_parent_child',\n",
    "    'grandparent_uncle_child',\n",
    "    'grandparent_child_child',\n",
    "    'child_child_sister',\n",
    "    'child_brother_uncle',\n",
    "    'child_child_brother',\n",
    "    'child_parent_grandparent',\n",
    "    'child_sister_aunt',\n",
    "    'child_spouse_parent',\n",
    "    'child_parent',\n",
    "    'child_aunt_brother',\n",
    "    'child_brother_parent',\n",
    "    'child_grandparent_child',\n",
    "    'child_sister_parent',\n",
    "    'parent_brother_parent',\n",
    "    'parent_child_grandparent',\n",
    "    'parent_parent_spouse',\n",
    "    'parent_sister_parent',\n",
    "    'parent_aunt_sister',\n",
    "    'parent_brother_child',\n",
    "    'parent_grandparent_parent',\n",
    "    'parent_sister_child',\n",
    "    'parent_uncle_brother',\n",
    "    'parent_child',\n",
    "    'parent_spouse_child'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict((clean_data/5).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.columns = all_cols\n",
    "values = (clean_data).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full = pd.concat([filter_questions,clean_data],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full.groupby(by=['Nationality','Native_Language']).agg(['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-developer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_weights = dict((clean_data/5).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-child",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('..','data','predicate_weights.json'),'w') as f:\n",
    "    json.dump(predicate_weights,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict = {'Français':'French','Française':'French'}\n",
    "\n",
    "filter_questions = filter_questions.replace(replace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-disability",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import spearmanr\n",
    "\n",
    "# rho, _ = spearmanr(clean_data[filter_questions['Native_Language']=='French'].mean(axis=0),\n",
    "#                   clean_data[filter_questions['Native_Language']=='English'].mean(axis=0))\n",
    "\n",
    "# print(rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "stat, p = mannwhitneyu(clean_data[filter_questions['Native_Language']=='French'].mean(axis=0),\n",
    "                       clean_data[filter_questions['Native_Language']=='English'].mean(axis=0))\n",
    "\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "    print('Probably the same distribution')\n",
    "else:\n",
    "    print('Probably different distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-formation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "stat, p = ttest_ind(clean_data[filter_questions['Native_Language']=='French'].mean(axis=0),\n",
    "                       clean_data[filter_questions['Native_Language']=='English'].mean(axis=0))\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "    print('Probably the same distribution')\n",
    "else:\n",
    "    print('Probably different distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot(values):\n",
    "    \n",
    "    std_error = np.std(values, ddof=1) / np.sqrt(len(values))\n",
    "    \n",
    "    ticks = np.arange(len(values))\n",
    "    \n",
    "    explanations = []\n",
    "\n",
    "    for i in values.index:\n",
    "\n",
    "        split_str = i.split('_')\n",
    "\n",
    "        relation = split_str[0]\n",
    "        \n",
    "        explanations.append('_'.join(split_str[1:]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "\n",
    "    ax.bar(x=ticks, \n",
    "           height=values,\n",
    "           yerr=std_error,\n",
    "          capsize=4)\n",
    "    \n",
    "    ax.set_xticks(ticks=ticks)\n",
    "    ax.set_xticklabels(labels=explanations,rotation = (45), fontsize = 10)\n",
    "    \n",
    "    ax.set_title(relation.title())\n",
    "    plt.savefig(os.path.join('..','plots',f\"{relation}.pdf\"),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot(values[0:4]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot(values[4:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-prior",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot(values[8:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot(values[10:18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot(values[18:27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot(values[27:35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot(values[35:46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot(values[46:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-guidance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "extra-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-offense",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "illegal-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'paul_dataset'\n",
    "RULE = 'spouse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "married-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(os.path.join('..','data',DATASET+'.npz'))\n",
    "\n",
    "triples,traces,weights,entities,relations = utils.get_data(data,RULE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "completed-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ENTITIES = len(entities)\n",
    "NUM_RELATIONS = len(relations)\n",
    "\n",
    "ent2idx = dict(zip(entities, range(NUM_ENTITIES)))\n",
    "rel2idx = dict(zip(relations, range(NUM_RELATIONS)))\n",
    "\n",
    "idx2ent = dict(zip(range(NUM_ENTITIES),entities))\n",
    "idx2rel = dict(zip(range(NUM_RELATIONS),relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nominated-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevance_scores = {}\n",
    "\n",
    "# for i in range(len(traces)):\n",
    "    \n",
    "#     stripped_trace = utils.remove_padding_np(traces[i])\n",
    "    \n",
    "# #     if str(stripped_trace) in relevance_scores:\n",
    "# #         print(stripped_trace)\n",
    "        \n",
    "#     predicate = '_'.join(stripped_trace[:,1])\n",
    "\n",
    "#     relevance_scores[str(stripped_trace)] = {predicate:weights[i]}\n",
    "    \n",
    "# relevance_scores[str(utils.remove_padding_np(traces[i]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "injured-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_scores = utils.get_relevance_scores(\n",
    "    utils.array2idx(traces,ent2idx,rel2idx),\n",
    "    weights,\n",
    "    ent2idx['UNK_ENT'],\n",
    "    rel2idx['UNK_REL']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "tender-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_traces = np.unique(traces.reshape(-1,3),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-discount",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "weird-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(flatten_traces)):\n",
    "    \n",
    "    is_duplicate = (flatten_traces[i] == flatten_traces).all(axis=1).sum()\n",
    "    \n",
    "    if is_duplicate >= 2:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-uruguay",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cultural-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_exp = utils.remove_padding_np(\n",
    "    utils.array2idx(traces[3],ent2idx,rel2idx),\n",
    "    ent2idx['UNK_ENT'],\n",
    "    rel2idx['UNK_REL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_g_precision(true_exp,pred_exp,relevance_scores):\n",
    "    \n",
    "#     n = len(pred_exp)\n",
    "\n",
    "#     if np.issubdtype(true_exp.dtype, np.integer):\n",
    "#         relation_key = '_'.join([str(i) for i in true_exp[:,1]])\n",
    "#     else:\n",
    "#         relation_key = '_'.join(true_exp[:,1])\n",
    "        \n",
    "#     g_precision = 0.0\n",
    "    \n",
    "#     for i in range(n):\n",
    "        \n",
    "#         triple = pred_exp[i]\n",
    "        \n",
    "#         rel_score = relevance_scores[str(triple)][relation_key]\n",
    "        \n",
    "#         g_precision += rel_score\n",
    "        \n",
    "#     return g_precision / n\n",
    "\n",
    "def get_g_recall(pred_exp,relevance_scores):\n",
    "    \n",
    "    pred_relevance = 0.0\n",
    "    total_relevance = relevance_scores.values()\n",
    "    \n",
    "    for i in range(len(pred_exp)):\n",
    "        \n",
    "        triple = pred_exp[i]\n",
    "        \n",
    "        rel_score = relevance_scores[str(triple)]\n",
    "        \n",
    "        pred_relevance += rel_score\n",
    "        \n",
    "    return pred_relevance / (total_relevance - pred_relevance)\n",
    "\n",
    "def get_g_recall(pred_exp,relevance_scores):\n",
    "    \n",
    "    pred_relevance = 0.0\n",
    "    total_relevance = relevance_scores.values()\n",
    "    \n",
    "    for i in range(len(pred_exp)):\n",
    "        \n",
    "        triple = pred_exp[i]\n",
    "        \n",
    "        rel_score = relevance_scores[str(triple)]\n",
    "        \n",
    "        pred_relevance += rel_score\n",
    "        \n",
    "    return pred_relevance / total_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-dependence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-cassette",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-upset",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
